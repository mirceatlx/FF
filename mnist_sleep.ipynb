{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "164a8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import wandb\n",
    "from ff import FF, FFLayer, FFEncoder\n",
    "from data import MNIST, MergedDataset\n",
    "from tqdm import tqdm\n",
    "# pip install lion-pytorch\n",
    "from lion_pytorch import Lion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c9bec64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60271c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_train = 512\n",
    "batch_size_test = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e80f3a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./datasets/MNIST/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "train_loader_negative = torch.utils.data.DataLoader(MergedDataset(torchvision.datasets.MNIST('./datasets/MNIST/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])), 5), batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f0f5681",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('./datasets/MNIST/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65901119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "squared_error = lambda x: x.pow(2).mean(1)\n",
    "deviation_error = lambda x: -((x - x.mean(1).unsqueeze(1)).pow(2).mean(1))\n",
    "for i in enumerate(train_loader_negative):\n",
    "    image = i[1].reshape((-1, 1, 28, 28))[0]\n",
    "    # visualize image with pil\n",
    "    from PIL import Image\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.imshow(image.reshape((28, 28)), cmap='gray')\n",
    "    break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bac009c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.5\n",
    "epochs_per_layer = 50\n",
    "model = FF(logging=False, device=device)\n",
    "optim_config = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "positive_optim_config = {\n",
    "    \"lr\": 0.0000625,\n",
    "\n",
    "}\n",
    "negative_optim_config = {\n",
    "    \"lr\": 0.001,\n",
    "}\n",
    "\n",
    "goodness_function = squared_error\n",
    "awake_period = 16\n",
    "sleep_period = 1\n",
    "\n",
    "model.add_layer(FFLayer(nn.Linear(784, 500).to(device), optimizer=torch.optim.Adam, epochs=epochs_per_layer, threshold=threshold, activation=nn.ReLU(), optim_config=optim_config, positive_optim_config=positive_optim_config, negative_optim_config=negative_optim_config, logging=False, name=\"layer 1\", device = device, goodness_function=goodness_function).to(device))\n",
    "model.add_layer(FFLayer(nn.Linear(500, 500).to(device), optimizer=torch.optim.Adam, epochs=epochs_per_layer, threshold=threshold, activation=nn.ReLU(), optim_config=optim_config, positive_optim_config=positive_optim_config, negative_optim_config=negative_optim_config, logging=False, name=\"layer 2\", device = device, goodness_function=goodness_function).to(device))\n",
    "model.add_layer(FFLayer(nn.Linear(500, 500).to(device), optimizer=torch.optim.Adam, epochs=epochs_per_layer, threshold=threshold, activation=nn.ReLU(), optim_config=optim_config, positive_optim_config=positive_optim_config, negative_optim_config=negative_optim_config, logging=False, name=\"layer 3\", device = device, goodness_function=goodness_function).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e781e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dfd050f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\rdyt2\\Desktop\\FF\\wandb\\run-20230223_235533-ax4n9agu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ffalgo/MNIST/runs/ax4n9agu' target=\"_blank\">normal-mask_negative_data-16-awake-1-sleep-0.0000625-0.001</a></strong> to <a href='https://wandb.ai/ffalgo/MNIST' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ffalgo/MNIST' target=\"_blank\">https://wandb.ai/ffalgo/MNIST</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ffalgo/MNIST/runs/ax4n9agu' target=\"_blank\">https://wandb.ai/ffalgo/MNIST/runs/ax4n9agu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wandb.init(project=\"MNIST\", entity=\"ffalgo\", name=\"normal-mask_negative_data-16-awake-1-sleep-0.0000625-0.001\")\n",
    "wandb.config = {\n",
    "  \"learning_rate\": 0.01,\n",
    "  \"awake_period\": awake_period,\n",
    "  \"sleep_period\": sleep_period,\n",
    "  \"epochs_per_layer\": epochs_per_layer,\n",
    "  \"batch_size\": 512,\n",
    "  \"activation\": \"relu\",\n",
    "  \"positive_lr\": 0.0000625,\n",
    "  \"negative_lr\": 0.0001,\n",
    "  \"threshold\": threshold,\n",
    "  \"optimizer\": torch.optim.Adam,\n",
    "  \"device\": device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "73f0956f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 28, 28])\n",
      "torch.Size([512, 28, 28])\n",
      "torch.Size([512, 28, 28])\n",
      "torch.Size([512, 28, 28])\n",
      "torch.Size([512, 28, 28])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m a, b \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39menumerate\u001b[39m(train_loader), \u001b[39menumerate\u001b[39m(train_loader_negative)):\n\u001b[0;32m      2\u001b[0m     x_neg \u001b[39m=\u001b[39m b[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(x_neg\u001b[39m.\u001b[39mshape)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\datasets\\mnist.py:145\u001b[0m, in \u001b[0;36mMNIST.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    142\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img\u001b[39m.\u001b[39mnumpy(), mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mL\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    144\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 145\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m    147\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[0;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torchvision\\transforms\\functional.py:171\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    169\u001b[0m img \u001b[39m=\u001b[39m img\u001b[39m.\u001b[39mpermute((\u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m    170\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(img, torch\u001b[39m.\u001b[39mByteTensor):\n\u001b[1;32m--> 171\u001b[0m     \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mto(dtype\u001b[39m=\u001b[39;49mdefault_float_dtype)\u001b[39m.\u001b[39;49mdiv(\u001b[39m255\u001b[39;49m)\n\u001b[0;32m    172\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    173\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for a, b in zip(enumerate(train_loader), enumerate(train_loader_negative)):\n",
    "    x_neg = b[1].to(device)\n",
    "    print(x_neg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "12e5ff4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [10:17:58<00:00, 74.16s/it] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Accuracy on train data</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Accuracy on test data</td><td>0.098</td></tr><tr><td>Accuracy on train data</td><td>0.09872</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">normal-mask_negative_data-16-awake-1-sleep-0.0000625-0.001</strong> at: <a href='https://wandb.ai/ffalgo/MNIST/runs/ax4n9agu' target=\"_blank\">https://wandb.ai/ffalgo/MNIST/runs/ax4n9agu</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20230223_235533-ax4n9agu\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "epochs = 500\n",
    "best_acc = 0.0\n",
    "hour = 0\n",
    "def get_random_number_besides(x):\n",
    "    import random\n",
    "    num = random.randint(0,9)\n",
    "    if num==x: return get_random_number_besides(x)\n",
    "    return num\n",
    "def get_negative_y(y):\n",
    "    return torch.tensor([get_random_number_besides(i) for i in y], dtype = torch.long).to(device)\n",
    "\n",
    "for i in tqdm(range(epochs)):\n",
    "    if i % 4 == 1:\n",
    "        predictions, real = MNIST.predict(test_loader, model, device)\n",
    "        acc = np.sum(predictions == real)/len(real)\n",
    "        wandb.log({\"Accuracy on test data\": acc})\n",
    "        if acc > best_acc and acc > 0.8:\n",
    "            best_acc = acc\n",
    "            # torch.save(model.state_dict(), 'best_mnist_80%.ph')\n",
    "        \n",
    "    predictions, real = MNIST.predict(train_loader, model, device)\n",
    "    acc = np.sum(predictions == real)/len(real)\n",
    "    wandb.log({\"Accuracy on train data\": acc})\n",
    "    model.train()\n",
    "    for a, b in zip(enumerate(train_loader), enumerate(train_loader_negative)):\n",
    "        x_pos = a[1][0].to(device)\n",
    "        y = a[1][1].to(device)\n",
    "        x_pos,_ = MNIST.overlay_y_on_x(x_pos, y)\n",
    "        x_neg,_ = MNIST.overlay_y_on_x(b[1].to(device), y)\n",
    "        if hour % (awake_period + sleep_period) < awake_period:\n",
    "            model.forward_positive(x_pos)\n",
    "        else:\n",
    "            model.forward_negative(x_neg)\n",
    "        # model.forward(x_pos, x_neg)\n",
    "        \n",
    "        hour += 1\n",
    "\n",
    "        \n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0012acc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "d6f8d400343bb3f57bd29c3eb1404dce9b47f9b5dee6203cb0f40ad0e600826b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
