{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6bd5542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from data import CIFAR10\n",
    "from ff import FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "764275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cfdc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c4cd4d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a894df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87d436b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bee61a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c9497",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d6eb7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"in_dims\": [3468, 3468, 3468],\n",
    "    \"out_dims\": [3468, 3468, 3468],\n",
    "    \"epochs\": 25,\n",
    "    \"threshold\": 2\n",
    "}\n",
    "num_layers = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cc54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "874e0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FF(num_layers, config, torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ef3c833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FF()"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "50b33d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10dac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5483bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x, new_y = CIFAR10.overlay_y_on_x(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6ce6aa96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
       "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
       "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
       "        0.0003, 0.0003, 0.0003, 0.0003, 0.0003], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3400d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "472887f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fa20358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 64,
=======
   "execution_count": 36,
>>>>>>> 897dd16 (actually added the code)
   "id": "3e413f52",
   "metadata": {},
   "outputs": [
    {
<<<<<<< HEAD
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0652529740333556\n",
      "0.805658061504364\n",
      "0.7595563681920369\n",
      "0.7343753457069399\n",
      "0.7452436836560566\n",
      "0.7430813280741373\n",
      "0.741107897758484\n",
      "0.7171123115221659\n",
      "0.7306486908594767\n",
      "0.7193388223648073\n",
      "0.7178562800089519\n",
      "0.7211289350191752\n",
      "0.7116290450096131\n",
      "0.7185840527216594\n",
      "0.7059982458750408\n",
      "0.7167496172587077\n",
      "0.718355646133423\n",
      "0.7067909177144368\n",
      "0.7118754641215007\n",
      "0.7085748728116353\n",
      "0.7076041142145794\n",
      "0.7059276803334553\n",
      "0.706224820613861\n",
      "0.7041837191581726\n",
      "0.7101206588745117\n",
      "0.7029241935412088\n",
      "0.7044783226648966\n",
      "0.7009640653928121\n",
      "0.7011525479952495\n",
      "0.7011675540606181\n"
     ]
    },
    {
=======
>>>>>>> 897dd16 (actually added the code)
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
<<<<<<< HEAD
      "\u001b[1;32m/home/azureuser/cloudfiles/code/FF/cifar.ipynb Cell 21\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y121sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m rnd \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandperm(x\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y121sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m x_neg, _ \u001b[39m=\u001b[39m CIFAR10\u001b[39m.\u001b[39moverlay_y_on_x(x, y[rnd])\n\u001b[0;32m----> <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y121sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m losses \u001b[39m=\u001b[39m model(x_pos, x_neg)\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y121sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(losses)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/ff.py:87\u001b[0m, in \u001b[0;36mFF.forward\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(x_pos)\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m---> 87\u001b[0m     x_pos, x_neg, loss \u001b[39m=\u001b[39m layer(x_pos, x_neg)\n\u001b[1;32m     88\u001b[0m     losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mmean(losses)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/ff.py:43\u001b[0m, in \u001b[0;36mFFLayer.forward\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     41\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     42\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n\u001b[0;32m---> 43\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptim\u001b[39m.\u001b[39;49mzero_grad()\n\u001b[1;32m     44\u001b[0m     pos_good \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoodness(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(x_pos))\n\u001b[1;32m     45\u001b[0m     neg_good \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgoodness(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall(x_neg))\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/optim/optimizer.py:245\u001b[0m, in \u001b[0;36mOptimizer.zero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    243\u001b[0m     p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mrequires_grad_(\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    244\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mnot\u001b[39;00m foreach \u001b[39mor\u001b[39;00m p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mis_sparse):\n\u001b[0;32m--> 245\u001b[0m     p\u001b[39m.\u001b[39;49mgrad\u001b[39m.\u001b[39;49mzero_()\n\u001b[1;32m    246\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     per_device_and_dtype_grads[p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdevice][p\u001b[39m.\u001b[39mgrad\u001b[39m.\u001b[39mdtype]\u001b[39m.\u001b[39mappend(p\u001b[39m.\u001b[39mgrad)\n",
=======
      "\u001b[1;32m/home/azureuser/cloudfiles/code/FF/cifar.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m x_neg, _ \u001b[39m=\u001b[39m CIFAR10\u001b[39m.\u001b[39moverlay_y_on_x(x\u001b[39m.\u001b[39mclone()\u001b[39m.\u001b[39mdetach(), false_y)\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m losses \u001b[39m=\u001b[39m model(x_pos, x_neg)\n\u001b[0;32m----> <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mprint\u001b[39m(CIFAR10\u001b[39m.\u001b[39;49mpredict(testloader, model))\n\u001b[1;32m      <a href='vscode-notebook-cell://amlext%2B2f737562736372697074696f6e732f63326265373765662d633166322d346533622d393938352d6531633837656362303230352f7265736f7572636547726f7570732f6d792d67726f75702f70726f7669646572732f4d6963726f736f66742e4d616368696e654c6561726e696e6753657276696365732f776f726b7370616365732f466f7277617264466f72776172642f636f6d70757465732f6573736578/home/azureuser/cloudfiles/code/FF/cifar.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mprint\u001b[39m(losses)\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/data.py:30\u001b[0m, in \u001b[0;36mCIFAR10.predict\u001b[0;34m(data_loader, model)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m num \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m10\u001b[39m):\n\u001b[1;32m     29\u001b[0m     over_x \u001b[39m=\u001b[39m CIFAR10\u001b[39m.\u001b[39moverlay_y_on_x(x, y)\n\u001b[0;32m---> 30\u001b[0m     good \u001b[39m=\u001b[39m model(over_x[\u001b[39m0\u001b[39;49m])\n\u001b[1;32m     31\u001b[0m     goodness\u001b[39m.\u001b[39mappend(good\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     32\u001b[0m predictions\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39margmax(goodness))\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/ff.py:83\u001b[0m, in \u001b[0;36mFF.forward\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     80\u001b[0m losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     82\u001b[0m \u001b[39mif\u001b[39;00m x_neg \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 83\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(x_pos)\n\u001b[1;32m     85\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[1;32m     86\u001b[0m     x_pos, x_neg, loss \u001b[39m=\u001b[39m layer(x_pos, x_neg)\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/ff.py:95\u001b[0m, in \u001b[0;36mFF.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     93\u001b[0m goodness \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m])\n\u001b[1;32m     94\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers):\n\u001b[0;32m---> 95\u001b[0m     x \u001b[39m=\u001b[39m layer(x)\n\u001b[1;32m     96\u001b[0m     goodness \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m layer\u001b[39m.\u001b[39mgoodness(x)\n\u001b[1;32m     97\u001b[0m \u001b[39mreturn\u001b[39;00m goodness\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/ff.py:37\u001b[0m, in \u001b[0;36mFFLayer.forward\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x_pos: torch\u001b[39m.\u001b[39mTensor, x_neg: torch\u001b[39m.\u001b[39mTensor \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     36\u001b[0m     \u001b[39mif\u001b[39;00m x_neg \u001b[39m==\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 37\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcall(x_pos)\n\u001b[1;32m     38\u001b[0m     losses \u001b[39m=\u001b[39m []\n\u001b[1;32m     39\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepochs):\n",
      "File \u001b[0;32m/mnt/batch/tasks/shared/LS_root/mounts/clusters/essex/code/FF/ff.py:31\u001b[0m, in \u001b[0;36mFFLayer.call\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m x_norm \u001b[39m=\u001b[39m x_norm \u001b[39m+\u001b[39m \u001b[39m1e-4\u001b[39m\n\u001b[1;32m     30\u001b[0m x_dir \u001b[39m=\u001b[39m x \u001b[39m/\u001b[39m x_norm\n\u001b[0;32m---> 31\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(x_dir)\n\u001b[1;32m     32\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/anaconda/envs/azureml_py38_PT_TF/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
>>>>>>> 897dd16 (actually added the code)
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i, (x, y) in enumerate(trainloader):\n",
    "    \n",
    "    x_pos, _ =  CIFAR10.overlay_y_on_x(x, y)\n",
    "    rnd = torch.randperm(x.size(0))\n",
    "    x_neg, _ = CIFAR10.overlay_y_on_x(x, y[rnd])\n",
    "    losses = model(x_pos, x_neg)\n",
<<<<<<< HEAD
    "    print(losses)"
=======
    "    print(CIFAR10.predict(testloader, model))\n",
    "    print(losses)\n"
>>>>>>> 897dd16 (actually added the code)
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
