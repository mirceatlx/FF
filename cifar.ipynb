{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bd5542c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "from data import CIFAR10\n",
    "from ff import FF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764275e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cfdc01a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4cd4d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a894df7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87d436b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "testset = torchvision.datasets.CIFAR10(root='./datasets', train=False,\n",
    "                                       download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bee61a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119c9497",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6eb7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"in_dims\": [3072, 3072],\n",
    "    \"out_dims\": [3072, 3072],\n",
    "    \"epochs\": 4,\n",
    "    \"threshold\": 1.5\n",
    "}\n",
    "num_layers = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2cc54b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "874e0284",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FF(num_layers, config, torch.optim.Adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef3c833b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FF()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50b33d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iter(trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10dac873",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5483bf1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_x, new_y = CIFAR10.overlay_y_on_x(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6ce6aa96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3072])\n",
      "tensor([0.0001, 0.0001, 0.0001, 0.0001], grad_fn=<MeanBackward1>)\n",
      "torch.Size([4, 3072])\n",
      "tensor([0.0001, 0.0001, 0.0001, 0.0001], grad_fn=<MeanBackward1>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.0002, 0.0002, 0.0002, 0.0002], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(new_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3400d",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "472887f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "trainset = torchvision.datasets.CIFAR10(root='./datasets', train=True,\n",
    "                                        download=True, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa20358f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e413f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.950603\n",
      "0.943953\n",
      "0.9349706\n",
      "0.9205848\n",
      "0.9278462\n",
      "0.87681806\n",
      "0.8407248\n",
      "0.88461375\n",
      "0.76245344\n",
      "0.81595945\n",
      "0.8767848\n",
      "0.7442508\n",
      "0.80894935\n",
      "0.79518664\n",
      "0.80952275\n",
      "0.8305385\n",
      "0.8147986\n",
      "0.7591651\n",
      "0.7532683\n",
      "0.84610057\n",
      "0.81066287\n",
      "0.7875005\n",
      "0.7887848\n",
      "0.7958966\n",
      "0.7614354\n",
      "0.74856603\n",
      "0.811008\n",
      "0.779246\n",
      "0.82085335\n",
      "0.7566781\n",
      "0.7251791\n",
      "0.74040264\n",
      "0.72135925\n",
      "0.7518133\n",
      "0.7381878\n",
      "0.7568997\n",
      "0.7721082\n",
      "0.79780674\n",
      "0.73032856\n",
      "0.7198066\n",
      "0.7110294\n",
      "0.78091025\n",
      "0.7618953\n",
      "0.73737335\n",
      "0.7573011\n",
      "0.70215094\n",
      "0.72947335\n",
      "0.7776158\n",
      "0.7355968\n",
      "0.7448491\n",
      "0.7474524\n",
      "0.75043774\n",
      "0.74044687\n",
      "0.75440204\n",
      "0.7209739\n",
      "0.7123635\n",
      "0.7292723\n",
      "0.7366022\n",
      "0.7369691\n",
      "0.69742835\n",
      "0.72892314\n",
      "0.71825814\n",
      "0.7544435\n",
      "0.7089061\n",
      "0.7385582\n",
      "0.7159186\n",
      "0.7243202\n",
      "0.72447824\n",
      "0.739874\n",
      "0.71977174\n",
      "0.7202194\n",
      "0.76611745\n",
      "0.7774634\n",
      "0.74312603\n",
      "0.7436273\n",
      "0.7509624\n",
      "0.7627218\n",
      "0.72501063\n",
      "0.7333901\n",
      "0.74117035\n",
      "0.7415739\n",
      "0.73069996\n",
      "0.7420896\n",
      "0.7295407\n",
      "0.73329115\n",
      "0.7121377\n",
      "0.7008207\n",
      "0.73504794\n",
      "0.7468398\n",
      "0.72749114\n",
      "0.7163427\n",
      "0.70369244\n",
      "0.7218575\n",
      "0.7102656\n",
      "0.72839487\n",
      "0.7163131\n",
      "0.7295345\n",
      "0.7093854\n",
      "0.7359023\n",
      "0.71555734\n",
      "0.71866924\n",
      "0.7207952\n",
      "0.72871995\n",
      "0.7213964\n",
      "0.7093477\n",
      "0.7391919\n",
      "0.71473753\n",
      "0.7438173\n",
      "0.73367727\n",
      "0.74761313\n",
      "0.74134815\n",
      "0.727152\n",
      "0.70767975\n",
      "0.72908175\n",
      "0.70619786\n",
      "0.70192146\n",
      "0.70230114\n",
      "0.7026024\n",
      "0.72746205\n",
      "0.7215166\n",
      "0.7495634\n",
      "0.7049512\n",
      "0.7160944\n",
      "0.7304487\n",
      "0.71998036\n",
      "0.70754075\n",
      "0.6981528\n",
      "0.7182027\n",
      "0.704066\n",
      "0.721522\n",
      "0.6995415\n",
      "0.7050721\n",
      "0.72114885\n",
      "0.70598555\n",
      "0.70353746\n",
      "0.7109677\n",
      "0.7335878\n",
      "0.71010876\n",
      "0.69671506\n",
      "0.70820725\n",
      "0.72981375\n",
      "0.72030663\n",
      "0.70972955\n",
      "0.72405946\n",
      "0.71167433\n",
      "0.71386135\n",
      "0.6998205\n",
      "0.72342676\n",
      "0.72167146\n",
      "0.719754\n",
      "0.70893466\n",
      "0.7143906\n",
      "0.70023537\n",
      "0.7340642\n",
      "0.7010688\n",
      "0.7118638\n",
      "0.72259843\n",
      "0.7173874\n",
      "0.69990873\n",
      "0.70509386\n",
      "0.71188873\n",
      "0.71547675\n",
      "0.70327693\n",
      "0.72580934\n",
      "0.7314396\n",
      "0.7175661\n",
      "0.7134445\n",
      "0.7142494\n",
      "0.71297777\n",
      "0.7165902\n",
      "0.6986714\n",
      "0.70467\n",
      "0.7002926\n",
      "0.7326603\n",
      "0.7023326\n",
      "0.7162209\n",
      "0.71890664\n",
      "0.70081747\n",
      "0.70522\n",
      "0.7006557\n",
      "0.72907615\n",
      "0.7093269\n",
      "0.711394\n",
      "0.7142515\n",
      "0.7135385\n",
      "0.7179408\n",
      "0.70573545\n",
      "0.7140855\n",
      "0.7101359\n",
      "0.6998748\n",
      "0.7053013\n",
      "0.7147486\n",
      "0.71898496\n",
      "0.7020694\n",
      "0.72605395\n",
      "0.70350796\n",
      "0.7257673\n",
      "0.7286693\n",
      "0.7089211\n",
      "0.7115702\n",
      "0.7178419\n",
      "0.7166755\n",
      "0.7044134\n",
      "0.6999438\n",
      "0.6959121\n",
      "0.7111536\n",
      "0.6986599\n",
      "0.70899713\n",
      "0.70520055\n",
      "0.70333886\n",
      "0.7068278\n",
      "0.71659374\n",
      "0.7225995\n",
      "0.69880396\n",
      "0.70968556\n",
      "0.7192932\n",
      "0.7030344\n",
      "0.7118824\n",
      "0.72145784\n",
      "0.7080623\n",
      "0.70622325\n",
      "0.7168666\n",
      "0.71496737\n",
      "0.69769585\n",
      "0.7096589\n",
      "0.7130543\n",
      "0.7018859\n",
      "0.7025349\n",
      "0.69737566\n",
      "0.7287032\n",
      "0.7068809\n",
      "0.7143475\n",
      "0.7080064\n",
      "0.7092964\n",
      "0.6958667\n",
      "0.7140615\n",
      "0.69856274\n",
      "0.71085626\n",
      "0.7266703\n",
      "0.69992286\n",
      "0.7067563\n",
      "0.70280385\n",
      "0.705325\n",
      "0.6951691\n",
      "0.7027017\n",
      "0.71270084\n",
      "0.69979274\n",
      "0.7066215\n",
      "0.6989238\n",
      "0.704651\n",
      "0.6962824\n",
      "0.70555437\n",
      "0.72466254\n",
      "0.7071272\n",
      "0.7117382\n",
      "0.6984869\n",
      "0.7374548\n",
      "0.70267093\n",
      "0.7140953\n",
      "0.69420797\n",
      "0.71937406\n",
      "0.704967\n",
      "0.70769155\n",
      "0.6967437\n",
      "0.7017157\n",
      "0.7024737\n",
      "0.7141481\n",
      "0.6978959\n",
      "0.7067008\n",
      "0.7046747\n",
      "0.71116567\n",
      "0.6966461\n",
      "0.7100801\n",
      "0.71655107\n",
      "0.7063558\n",
      "0.7037858\n",
      "0.7214046\n",
      "0.71206206\n",
      "0.7053387\n",
      "0.70854175\n",
      "0.70468456\n",
      "0.7003855\n",
      "0.7021694\n",
      "0.70756817\n",
      "0.7032168\n",
      "0.70987284\n",
      "0.703944\n",
      "0.704479\n",
      "0.70705444\n",
      "0.6944946\n",
      "0.70482457\n",
      "0.69670415\n",
      "0.70811903\n",
      "0.7098454\n",
      "0.7041024\n",
      "0.6992889\n",
      "0.7017699\n",
      "0.70450085\n",
      "0.7037697\n",
      "0.7117244\n",
      "0.7022941\n",
      "0.70018303\n",
      "0.69894505\n",
      "0.71983516\n",
      "0.72388625\n",
      "0.7027143\n",
      "0.7050053\n",
      "0.69792545\n",
      "0.69920564\n",
      "0.7196394\n",
      "0.70360297\n",
      "0.70468795\n",
      "0.72444475\n",
      "0.69814473\n",
      "0.718312\n",
      "0.7096212\n",
      "0.7108063\n",
      "0.7073044\n",
      "0.7099385\n",
      "0.6970992\n",
      "0.7028096\n",
      "0.71603286\n",
      "0.6972879\n",
      "0.70066434\n",
      "0.6999771\n",
      "0.7010554\n",
      "0.70141125\n",
      "0.70726246\n",
      "0.6965971\n",
      "0.6986672\n",
      "0.7032314\n",
      "0.7071431\n",
      "0.6997751\n",
      "0.7045668\n",
      "0.69986355\n",
      "0.6998581\n",
      "0.7270576\n",
      "0.69836\n",
      "0.7085953\n",
      "0.6979759\n",
      "0.69803774\n",
      "0.7153233\n",
      "0.7083494\n",
      "0.6976944\n",
      "0.6970167\n",
      "0.6981329\n",
      "0.69751304\n",
      "0.7067087\n",
      "0.7050847\n",
      "0.7084056\n",
      "0.71052384\n",
      "0.69896436\n",
      "0.7199683\n",
      "0.6949705\n",
      "0.6982064\n",
      "0.7203417\n",
      "0.70143175\n",
      "0.69703364\n",
      "0.6982877\n",
      "0.69847673\n",
      "0.6943302\n",
      "0.70592135\n",
      "0.70230305\n",
      "0.70463455\n",
      "0.7007655\n",
      "0.69526243\n",
      "0.70627195\n",
      "0.69849133\n",
      "0.7015333\n",
      "0.7080086\n",
      "0.7002691\n",
      "0.69771624\n",
      "0.69877905\n",
      "0.701712\n",
      "0.69762576\n",
      "0.71467954\n",
      "0.69870293\n",
      "0.71549475\n",
      "0.6960156\n",
      "0.70086277\n",
      "0.6965389\n",
      "0.6989771\n",
      "0.70401406\n",
      "0.70387745\n",
      "0.70133126\n",
      "0.6962224\n",
      "0.69696033\n",
      "0.6972812\n",
      "0.7009822\n",
      "0.7040471\n",
      "0.70236826\n",
      "0.7089814\n",
      "0.71060884\n",
      "0.69821787\n",
      "0.70716625\n",
      "0.7012083\n",
      "0.7090006\n",
      "0.6976877\n",
      "0.723788\n",
      "0.6963568\n",
      "0.719082\n",
      "0.70737183\n",
      "0.7161461\n",
      "0.70022124\n",
      "0.70804286\n",
      "0.70630187\n",
      "0.70259464\n",
      "0.69835556\n",
      "0.70030797\n",
      "0.6948949\n",
      "0.6997154\n",
      "0.6989176\n",
      "0.7006843\n",
      "0.6996644\n",
      "0.70702416\n",
      "0.69805646\n",
      "0.6944386\n",
      "0.69945776\n",
      "0.6968484\n",
      "0.69588053\n",
      "0.69675624\n",
      "0.70352304\n",
      "0.7002608\n",
      "0.70296955\n",
      "0.71123874\n",
      "0.69699496\n",
      "0.7001444\n",
      "0.7031683\n",
      "0.6950168\n",
      "0.70792913\n",
      "0.70302343\n",
      "0.69690907\n",
      "0.69484293\n",
      "0.71033543\n",
      "0.7087641\n",
      "0.7014951\n",
      "0.7005775\n",
      "0.7051158\n",
      "0.6991557\n",
      "0.7000642\n",
      "0.70294917\n",
      "0.7008018\n",
      "0.70205295\n",
      "0.69481033\n",
      "0.6973941\n",
      "0.70166683\n",
      "0.70120263\n",
      "0.70067394\n",
      "0.6964845\n",
      "0.6995968\n",
      "0.69917953\n",
      "0.7030309\n",
      "0.6954249\n",
      "0.7005023\n",
      "0.70694846\n",
      "0.70258176\n",
      "0.69594014\n",
      "0.70044184\n",
      "0.6992916\n",
      "0.69859236\n",
      "0.6971314\n",
      "0.7010354\n",
      "0.7036499\n",
      "0.6996399\n",
      "0.70834523\n",
      "0.69906664\n",
      "0.6958365\n",
      "0.71203977\n",
      "0.7033713\n",
      "0.7029331\n",
      "0.6961874\n",
      "0.6971886\n",
      "0.7044147\n",
      "0.7061323\n",
      "0.6987515\n",
      "0.6944991\n",
      "0.7080032\n",
      "0.697454\n",
      "0.6977714\n",
      "0.70208895\n",
      "0.70574087\n",
      "0.6975063\n",
      "0.69401646\n",
      "0.69545037\n",
      "0.7057346\n",
      "0.70326746\n",
      "0.69956636\n",
      "0.69716394\n",
      "0.7200788\n",
      "0.7009611\n",
      "0.69961613\n",
      "0.7007749\n",
      "0.6962612\n",
      "0.6975677\n",
      "0.6992159\n",
      "0.69613844\n",
      "0.7023835\n",
      "0.6985024\n",
      "0.70363265\n",
      "0.69551176\n",
      "0.70161146\n",
      "0.7000412\n",
      "0.6954553\n",
      "0.69990396\n",
      "0.69813395\n",
      "0.69897115\n",
      "0.7032357\n",
      "0.69842803\n",
      "0.7035073\n",
      "0.70283306\n",
      "0.7042839\n",
      "0.70080805\n",
      "0.70007926\n",
      "0.6994196\n",
      "0.6972468\n",
      "0.695332\n",
      "0.69662887\n",
      "0.6998231\n",
      "0.70335436\n",
      "0.6958683\n",
      "0.7018597\n",
      "0.70951194\n",
      "0.7095677\n",
      "0.6958252\n",
      "0.7047555\n",
      "0.69705594\n",
      "0.70094055\n",
      "0.6981826\n",
      "0.6955347\n",
      "0.6967889\n",
      "0.69711065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x115236670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/multiprocessing/popen_fork.py\", line 44, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ww/jr0958_14cn18kjs05m9k57c0000gq/T/ipykernel_13620/3038627068.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mfalse_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCIFAR10\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moverlay_y_on_x\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfalse_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/FF/ff.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m        \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m            \u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_neg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m            \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1480\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1481\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1482\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/research/FF/ff.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_pos, x_neg)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mmloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mmloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             )\n\u001b[0;32m--> 484\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    485\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m         )\n",
      "\u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/deeplearning/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "for i, (x, y) in enumerate(trainloader):\n",
    "    \n",
    "    x_pos, _ =  CIFAR10.overlay_y_on_x(x, y)\n",
    "    false_y = torch.randint(0, 10, (x.shape[0],))\n",
    "    x_neg, _ = CIFAR10.overlay_y_on_x(x, false_y)\n",
    "    losses = model(x_pos, x_neg)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "053f0e75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
